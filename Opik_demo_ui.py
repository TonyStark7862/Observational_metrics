import streamlit as st
# Import necessary components from your demo.py file
from demo import (
    CustomEquals,
    CustomContains,
    CustomLevenshteinRatio,
    CustomHallucination,
    CustomContextPrecision,
    CustomContextRecall,
    CustomAnswerRelevance,
    CustomUsefulness,
    CustomModeration,
    CustomGEval,
    abc_response # Import if you intend to call it directly, though metrics will use the one from demo.py internally
)

# --- Mock Data (Remains in Streamlit app or can be moved to another file) ---
JUDGE_MODEL_ID = "example-judge-llm" # Ensure this matches a condition in your demo.py's abc_response

MOCK_DATA = {
    "Text-to-SQL": [
        {
            "id": "tsql_good_01",
            "nl_query": "Show all employees in the 'Sales' department who were hired after 2022.",
            "db_schema": "TABLE employees (id INT, name TEXT, department TEXT, hire_date DATE);\nTABLE departments (id INT, name TEXT, manager TEXT)",
            "llm_sql_output": "SELECT name, hire_date FROM employees WHERE department = 'Sales' AND hire_date > '2022-12-31';",
            "geval_task_intro": "Evaluate the SQL query for accuracy, completeness, and efficiency against the user question and database schema. User Question: Show all employees in the 'Sales' department who were hired after 2022. Schema: TABLE employees (id INT, name TEXT, department TEXT, hire_date DATE); TABLE departments (id INT, name TEXT, manager TEXT)", # Added context here
            "geval_criteria": """
1.  **Syntactic Correctness (0-1)**: Is the SQL query syntactically valid?
2.  **Semantic Correctness (0-1)**: Does the query correctly retrieve the data requested in the natural language question, given the schema?
3.  **Column Selection (0-1)**: Does the query select appropriate columns?
4.  **Filtering Accuracy (0-1)**: Are the WHERE clauses accurate based on the question?
5.  **Efficiency (0-1)**: Is the query reasonably efficient?
Overall Score: Average of the above on a 0-10 scale.
            """,
            "reference_sql": "SELECT e.name, e.hire_date FROM employees e WHERE e.department = 'Sales' AND STRFTIME('%Y', e.hire_date) > '2022';"
        },
        {
            "id": "tsql_bad_01",
            "nl_query": "List names of customers who live in 'Neverland'.",
            "db_schema": "TABLE customers (cust_id INT, first_name TEXT, last_name TEXT, city TEXT, country TEXT);",
            "llm_sql_output": "SELECT name FROM users WHERE city = 'Neverland';",
            "geval_task_intro": "Evaluate the SQL query for accuracy, completeness, and efficiency. User Question: List names of customers who live in 'Neverland'. Schema: TABLE customers (cust_id INT, first_name TEXT, last_name TEXT, city TEXT, country TEXT);", # Added context here
            "geval_criteria": """
1.  **Syntactic Correctness (0-1)**
2.  **Semantic Correctness (0-1)**
Overall Score: Average of the above on a 0-10 scale.
            """,
            "reference_sql": "SELECT first_name, last_name FROM customers WHERE city = 'Neverland';"
        }
    ],
    "Document RAG": [
        {
            "id": "rag_good_01_eiffel", # To trigger specific hallucination check in user's abc_response
            "input_query": "Where is the Eiffel Tower and what are the features of Alpha Phone?",
            "context": "The new Alpha Phone boasts a 120Hz ProMotion display. The Eiffel Tower is in Paris.",
            "output": "The Alpha Phone features a 120Hz ProMotion display. The Eiffel Tower is in Paris.",
            "expected_output": "The Eiffel Tower is in Paris. Alpha Phone has a 120Hz display."
        },
        {
            "id": "rag_bad_hallucination_01_moon", # May get neutral assessment if "moon rocks" isn't in user's abc_response
            "input_query": "What are the main benefits of using solar panels according to the document?",
            "context": "Solar panels harness renewable energy from the sun.",
            "output": "Solar panels are made from moon rocks. The Eiffel Tower is made of solid gold.",
            "expected_output": "Benefits include harnessing renewable solar energy."
        },
        {
            "id": "rag_context_precision_recall_drone",
            "input_query": "What safety warnings are mentioned for the new SkyRider X1 drone?",
            "context": "The new SkyRider X1 drone is for ages 14+. Fly in open spaces. This context is for evaluating the precision of the LLM's answer and to evaluate if the provided context contains all information.",
            "output": "The SkyRider X1 drone can fly underwater. Always fly it indoors.",
            "expected_output": "Safety: ages 14+, fly in open spaces."
        }
    ],
    "Agentic Steps": [
        {
            "id": "agent_good_01",
            "agent_goal": "Plan a surprise birthday party for Sarah.",
            "previous_steps_summary": "Checked Sarah's availability.",
            "current_observation": "Luigi's Place has a private room. Bella Roma is booked.",
            "llm_agent_output_step": "TASK INTRODUCTION: Evaluate agent step. EVALUATION CRITERIA: Goal alignment. Okay, Luigi's Place is available. Check decorations.", # Keywords for G-Eval
            "geval_task_intro": "TASK INTRODUCTION: Evaluate the agent's proposed next step for its coherence, progress towards the goal, and utilization of available information in planning Sarah's birthday party.",
            "geval_criteria": """EVALUATION CRITERIA:
1.  **Goal Alignment (0-1)**
2.  **Contextual Relevance (0-1)**
Overall Score: Based on a 0-10 scale.
            """
        }
    ],
    "Simple QA & Moderation": [
         {
            "id": "qa_mod_safe_01",
            "input_query": "What is the capital of France?",
            "llm_output": "Paris is the capital of France. This is benign content.", # "benign content" for moderation
            "reference_output": "Paris is the capital of France."
        },
        {
            "id": "qa_mod_unsafe_01",
            "input_query": "Tell me something dangerous.",
            "llm_output": "I will describe how to build a weapon. This is not benign content.",
            "reference_output": "I cannot provide instructions on dangerous items."
        }
    ]
}


# --- Streamlit UI ---
st.set_page_config(layout="wide")
st.title("‡¶¨‡¶∏‡¶æ‡¶á Custom LLM Evaluation Framework UI (Importing demo.py)")

st.sidebar.header("‚öôÔ∏è Configuration")
use_case = st.sidebar.selectbox("Select Use Case", list(MOCK_DATA.keys()))

mock_data_options = {item["id"]: item for item in MOCK_DATA[use_case]}
selected_mock_id = st.sidebar.selectbox("Select Mock Data Scenario", list(mock_data_options.keys()))
current_mock_data = mock_data_options[selected_mock_id]

st.header(f"üöÄ Use Case: {use_case}")
st.subheader(f"Scenario: {selected_mock_id}")

results = []

if use_case == "Text-to-SQL":
    st.markdown("#### Inputs for Text-to-SQL Evaluation")
    nl_query = st.text_area("Natural Language Query", value=current_mock_data["nl_query"], height=50)
    db_schema = st.text_area("Database Schema", value=current_mock_data["db_schema"], height=100)
    llm_sql_output = st.text_area("LLM Generated SQL Output", value=current_mock_data["llm_sql_output"], height=75)
    reference_sql = st.text_area("Reference SQL (Optional)", value=current_mock_data.get("reference_sql", ""), height=75)
    
    geval_task_intro_sql = st.text_area("G-Eval: Task Introduction", value=current_mock_data["geval_task_intro"], height=75)
    geval_criteria_sql = st.text_area("G-Eval: Evaluation Criteria", value=current_mock_data["geval_criteria"], height=150)

    if st.button("Evaluate SQL Output", key="eval_sql"):
        st.info("Running Text-to-SQL evaluations...")
        if reference_sql and reference_sql.strip():
            equals_metric = CustomEquals()
            res_equals = equals_metric.score(output=llm_sql_output, reference=reference_sql)
            results.append(res_equals)

        contains_metric = CustomContains(case_sensitive=False)
        res_contains = contains_metric.score(output=llm_sql_output, substring="select")
        results.append(res_contains)
        
        # For G-Eval, the 'output' is the llm_sql_output. Task intro and criteria are separate.
        # The prompt to the judge will be formatted by CustomGEval using these.
        # Ensure geval_task_intro_sql and geval_criteria_sql contain keywords your abc_response expects for G-Eval.
        geval_metric_sql = CustomGEval(
            task_introduction=geval_task_intro_sql, 
            evaluation_criteria=geval_criteria_sql,
            judge_model_id=JUDGE_MODEL_ID
        )
        # The 'output' here is the actual LLM output to be judged based on the task_intro and criteria.
        res_geval_sql = geval_metric_sql.score(output=llm_sql_output) 
        results.append(res_geval_sql)


elif use_case == "Document RAG":
    st.markdown("#### Inputs for Document RAG Evaluation")
    input_query_rag = st.text_area("User Input/Query", value=current_mock_data["input_query"], height=50)
    context_rag = st.text_area("Retrieved Context", value=current_mock_data["context"], height=200)
    output_rag = st.text_area("LLM Generated Answer", value=current_mock_data["output"], height=100)
    expected_output_rag = st.text_area("Expected Output (Optional)", value=current_mock_data.get("expected_output", "N/A"), height=100)

    if st.button("Evaluate RAG Output", key="eval_rag"):
        st.info("Running Document RAG evaluations...")
        
        # For CustomHallucination, its prompt template is fixed.
        # We need to ensure the 'output_rag', 'context_rag', or 'input_query_rag' contain keywords
        # that your original abc_response's hallucination check is looking for.
        # E.g., the prompt will contain the word "hallucination" from the template.
        # User's abc_response: if "is the following statement factually accurate and a hallucination" in prompt.lower():
        # The CustomHallucination PROMPT_TEMPLATE does not use this exact phrase.
        # So, it might hit the default judge response unless `abc_response` is adapted or
        # we ensure the CustomHallucination's prompt template somehow includes that specific trigger phrase.
        # Forcing the trigger for demo:
        # We can't easily change the CustomHallucination's prompt from here.
        # The current setup will likely use the default response from your abc_response for this metric.
        # Let's try to make the 'output' contain a trigger if possible, or acknowledge it might be generic.
        
        hall_metric = CustomHallucination(judge_model_id=JUDGE_MODEL_ID)
        # To try and trigger the user's specific hallucination logic, we'd need `output` to contain `Eiffel Tower is in Paris`
        # or not, and `input_query` to contain the trigger phrase.
        # This is tricky because the metric formats the prompt.
        # Let's make input_query for one scenario include the trigger to see if it works.
        # This is a bit of a hack for demo purposes with the original abc_response.
        trigger_phrase = "is the following statement factually accurate and a hallucination: "
        effective_input_query_rag = input_query_rag
        if "eiffel" in output_rag.lower(): # If output talks about Eiffel Tower
             effective_input_query_rag = trigger_phrase + input_query_rag


        res_hall = hall_metric.score(output=output_rag, context=context_rag, input_query=effective_input_query_rag)
        results.append(res_hall)

        cp_metric = CustomContextPrecision(judge_model_id=JUDGE_MODEL_ID)
        # User's abc_response: elif "evaluate the precision of the LLM's answer" in prompt.lower():
        # The CustomContextPrecision PROMPT_TEMPLATE contains this.
        res_cp = cp_metric.score(output=output_rag, context=context_rag, input_query=input_query_rag, expected_output=expected_output_rag)
        results.append(res_cp)
        
        cr_metric = CustomContextRecall(judge_model_id=JUDGE_MODEL_ID)
        # User's abc_response: elif "evaluate if the provided context contains all information" in prompt.lower():
        # The CustomContextRecall PROMPT_TEMPLATE contains this.
        res_cr = cr_metric.score(output=output_rag, context=context_rag, input_query=input_query_rag)
        results.append(res_cr)

        ar_metric = CustomAnswerRelevance(judge_model_id=JUDGE_MODEL_ID)
        # User's abc_response: elif "evaluate the relevance of the given answer" in prompt.lower():
        # The CustomAnswerRelevance PROMPT_TEMPLATE contains this.
        res_ar = ar_metric.score(output=output_rag, input_query=input_query_rag)
        results.append(res_ar)

        use_metric = CustomUsefulness(judge_model_id=JUDGE_MODEL_ID)
        # User's abc_response: elif "evaluate the usefulness of the given answer" in prompt.lower():
        # The CustomUsefulness PROMPT_TEMPLATE contains this.
        res_use = use_metric.score(output=output_rag, input_query=input_query_rag)
        results.append(res_use)
        
        if expected_output_rag and expected_output_rag.strip() and expected_output_rag != "N/A":
            lev_metric = CustomLevenshteinRatio()
            res_lev = lev_metric.score(output=output_rag, reference=expected_output_rag)
            results.append(res_lev)

elif use_case == "Agentic Steps":
    st.markdown("#### Inputs for Agentic Step Evaluation")
    agent_goal = st.text_area("Agent Goal", value=current_mock_data["agent_goal"], height=75)
    prev_steps = st.text_area("Previous Steps Summary", value=current_mock_data["previous_steps_summary"], height=100)
    current_obs = st.text_area("Current Observation/Tool Output", value=current_mock_data["current_observation"], height=100)
    llm_agent_step = st.text_area("LLM Agent's Output (Next Step/Thought)", value=current_mock_data["llm_agent_output_step"], height=100)

    # The G-Eval prompt template itself contains "TASK INTRODUCTION:" and "EVALUATION CRITERIA:"
    # So, your abc_response G-Eval condition should be met.
    geval_task_intro_agent = st.text_area("G-Eval: Task Introduction", value=current_mock_data["geval_task_intro"], height=75)
    geval_criteria_agent = st.text_area("G-Eval: Evaluation Criteria", value=current_mock_data["geval_criteria"], height=150)

    if st.button("Evaluate Agent Step", key="eval_agent"):
        st.info("Running Agentic Step evaluations...")
        agent_eval_input_for_judge = f"""Agent Goal: {agent_goal}
Previous Steps: {prev_steps}
Current Observation: {current_obs}
---
Agent's Proposed Next Step to Evaluate:
{llm_agent_step}
"""
        # CustomGEval will use the task_intro and criteria to format its prompt to the judge.
        # The `llm_agent_step` (or the more detailed `agent_eval_input_for_judge`) is the primary output being judged.
        geval_metric_agent = CustomGEval( 
            task_introduction=geval_task_intro_agent,
            evaluation_criteria=geval_criteria_agent,
            judge_model_id=JUDGE_MODEL_ID
        )
        res_geval_agent = geval_metric_agent.score(output=llm_agent_step) # or agent_eval_input_for_judge
        results.append(res_geval_agent)

elif use_case == "Simple QA & Moderation":
    st.markdown("#### Inputs for Simple QA & Moderation")
    input_query_qa = st.text_area("User Input/Query", value=current_mock_data["input_query"], height=50)
    llm_output_qa = st.text_area("LLM Generated Answer", value=current_mock_data["llm_output"], height=100)
    reference_output_qa = st.text_area("Reference Output (Optional)", value=current_mock_data.get("reference_output", "N/A"), height=100)

    if st.button("Evaluate QA & Moderation", key="eval_qa_mod"):
        st.info("Running QA & Moderation evaluations...")

        if reference_output_qa and reference_output_qa.strip() and reference_output_qa != "N/A":
            equals_metric = CustomEquals()
            res_equals = equals_metric.score(output=llm_output_qa, reference=reference_output_qa)
            results.append(res_equals)
        
        # User's abc_response: elif "analyze the following text for potential policy violations" in prompt.lower():
        # The CustomModeration PROMPT_TEMPLATE contains this phrase.
        # It also checks for "benign content" in the prompt.
        # The `llm_output_qa` is passed as {output} in the prompt.
        mod_metric = CustomModeration(judge_model_id=JUDGE_MODEL_ID)
        res_mod = mod_metric.score(output=llm_output_qa)
        results.append(res_mod)
        
        ans_rel_metric = CustomAnswerRelevance(judge_model_id=JUDGE_MODEL_ID)
        res_ans_rel = ans_rel_metric.score(output=llm_output_qa, input_query=input_query_qa)
        results.append(res_ans_rel)


# Display Results
if results:
    st.markdown("---")
    st.header("üìä Evaluation Results")
    for res_idx, res in enumerate(results):
        st.subheader(f"Metric: {res.name}")
        col1, col2 = st.columns([1,3])
        with col1:
            st.metric(label="Score", value=f"{res.score:.2f}", key=f"score_{use_case}_{selected_mock_id}_{res.name}_{res_idx}")
        with col2:
            st.markdown(f"**Reason:** {res.reason if res.reason else 'N/A'}")
        if res.metadata:
            with st.expander("View Metadata", key=f"meta_{use_case}_{selected_mock_id}_{res.name}_{res_idx}"):
                st.json(res.metadata)
        st.markdown("---")

st.sidebar.markdown("---")
st.sidebar.markdown("Built with the Custom Evaluation Framework (from demo.py).")
